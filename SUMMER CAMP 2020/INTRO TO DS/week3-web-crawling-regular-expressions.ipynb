{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🦉 COSADAMA  Introduction to Data Science Study\n",
    "* 일자: 2020-07-22\n",
    "* 작성자: 김가윤\n",
    "* 참고자료: 파이썬으로 데이터 주무르기(민형기), 점프 투 파이썬(박응용), 파이썬 입문과 크롤링 기초 부트캠프(잔재미코딩), 하람님 교안\n",
    "* 교재: 125 - 138쪽 (깃헙 교안으로 공부하면서 교재를 참고 삼아, 복습 겸 읽어보세요~)\n",
    "* 학습키워드: 웹크롤링, 정규표현식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 웹 크롤링과 정규표현식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 학습 목표\n",
    "    1. **BeatifulSoup과 urllib 라이브러리를 활용해 웹 크롤링**을 해보고, 엑셀을 열고 기록하는데 사용되는 모듈 openpyxl을 사용해 가져온 타이틀들을 엑셀 파일에 저장하고 읽어 봅니다.\n",
    "    2. **정규표현식을 통해 원하는 데이터를 추출**합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 웹 크롤링과 정규표현식이란?\n",
    "\n",
    "웹 크롤링이라는 것은 웹에서 내가 원하는 데이터를 긁어오는 것을 말합니다. 웹 크롤링 기술에도 여러가지가 있기 때문에 이번 시간에는 기초적인 기술을 먼저 배워볼거에요! 크롤링한 데이터를 내가 원하는 부분만 가져오기 위해 정규표현식이 쓰이는거랍니다 :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 HTML/CSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "웹 크롤링을 하려면 웹이 어떤 문법으로 만들어지는지부터 우선 알아야 합니다. html는 웹 페이지를 만들기 위한 언어로 웹브라우저 위에서 동작하는 언어에요. 웹 페이지 뿐만 아니라 이미지, 텍스트, 비디오의 내용도 담을 수 있습니다.  CSS는 간단히 웹 브라우저를 꾸며주는 언어라고 생각하면 되구요. 웹이 어떻게 구성되어 있는지를 이해해야 웹 크롤링도 할 수 있어요. 아래 링크를 보고 해당 부분만 학습하고 오면 됩니다! \n",
    "\n",
    "* [Introduction to HTML](https://www.codecademy.com/learn/learn-html) - syllabus 1. elements & structured의 Introduction to HTML과 HTML Document Standard 학습하기\n",
    "* [Learn CSS](https://www.codecademy.com/learn/learn-css) - syllabus 1. Selectors and Visual Rules의 CSS Setup & Selectors 학습하기\n",
    "\n",
    "처음 배우시면 이것을 끝내시는데 하루(3시간 이상) 정도는 걸릴 것 같아요. html/css를 아시는 분들은 이 파트를 넘어가시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 웹 데이터를 가져오는 BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup은 웹 페이지의 내용을 가져오기 위해 사용하는 모듈이에요. \n",
    "[BeautifulSoup에 대한 자세한 내용](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)은 링크를 클릭해주세요! \n",
    "\n",
    "우선 구글 드라이브의 **02. test_first.html**을 다운받아 직접 학습해봅시다.\n",
    "먼저 BeautifulSoup을 import 해줄게요~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 다운 받은 파일을 열어 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "\n",
      "<html>\n",
      "<head>\n",
      "<title>Very Simple HTML Code by PinkWink</title>\n",
      "</head>\n",
      "<body>\n",
      "<div>\n",
      "<p class=\"inner-text first-item\" id=\"first\">\n",
      "                Happy PinkWink.\n",
      "                <a href=\"http://www.pinkwink.kr\" id=\"pw-link\">PinkWink</a>\n",
      "</p>\n",
      "<p class=\"inner-text second-item\">\n",
      "                Happy Data Science.\n",
      "                <a href=\"https://www.python.org\" id=\"py-link\">Python</a>\n",
      "</p>\n",
      "</div>\n",
      "<p class=\"outer-text first-item\" id=\"second\">\n",
      "<b>\n",
      "                Data Science is funny.\n",
      "            </b>\n",
      "</p>\n",
      "<p class=\"outer-text\">\n",
      "<b>\n",
      "                All I need is Love.\n",
      "            </b>\n",
      "</p>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "page = open(\"C:\\\\Users\\\\Owner\\\\Documents\\\\GitHub\\\\cosadama\\\\SUMMER CAMP 2020\\\\INTRO TO DS\\\\03. test_first.html\", \"r\").read()\n",
    "# 본인 파일 경로 넣기\n",
    "soup = BeautifulSoup(page, 'html.parser') # Python’s html.parser - 문서 전체를 저장한 변수 \n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "페이지를 이루고 있는 html의 구조가 불러져왔어요~ 더 예쁘게 보고 싶다면 `soup.prettify()`를 이용하면 됩니다.\n",
    "\n",
    "* `soup.prettify()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   Very Simple HTML Code by PinkWink\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <div>\n",
      "   <p class=\"inner-text first-item\" id=\"first\">\n",
      "    Happy PinkWink.\n",
      "    <a href=\"http://www.pinkwink.kr\" id=\"pw-link\">\n",
      "     PinkWink\n",
      "    </a>\n",
      "   </p>\n",
      "   <p class=\"inner-text second-item\">\n",
      "    Happy Data Science.\n",
      "    <a href=\"https://www.python.org\" id=\"py-link\">\n",
      "     Python\n",
      "    </a>\n",
      "   </p>\n",
      "  </div>\n",
      "  <p class=\"outer-text first-item\" id=\"second\">\n",
      "   <b>\n",
      "    Data Science is funny.\n",
      "   </b>\n",
      "  </p>\n",
      "  <p class=\"outer-text\">\n",
      "   <b>\n",
      "    All I need is Love.\n",
      "   </b>\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify())  # 태그를 구분하기 쉽게 해주는 명령"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['html',\n",
       " '\\n',\n",
       " <html>\n",
       " <head>\n",
       " <title>Very Simple HTML Code by PinkWink</title>\n",
       " </head>\n",
       " <body>\n",
       " <div>\n",
       " <p class=\"inner-text first-item\" id=\"first\">\n",
       "                 Happy PinkWink.\n",
       "                 <a href=\"http://www.pinkwink.kr\" id=\"pw-link\">PinkWink</a>\n",
       " </p>\n",
       " <p class=\"inner-text second-item\">\n",
       "                 Happy Data Science.\n",
       "                 <a href=\"https://www.python.org\" id=\"py-link\">Python</a>\n",
       " </p>\n",
       " </div>\n",
       " <p class=\"outer-text first-item\" id=\"second\">\n",
       " <b>\n",
       "                 Data Science is funny.\n",
       "             </b>\n",
       " </p>\n",
       " <p class=\"outer-text\">\n",
       " <b>\n",
       "                 All I need is Love.\n",
       "             </b>\n",
       " </p>\n",
       " </body>\n",
       " </html>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(soup.children) # soup에서 children(한단계 아래인 애들) 인 부분을 묶어서 리스트로 만들어줘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html>\n",
       "<head>\n",
       "<title>Very Simple HTML Code by PinkWink</title>\n",
       "</head>\n",
       "<body>\n",
       "<div>\n",
       "<p class=\"inner-text first-item\" id=\"first\">\n",
       "                Happy PinkWink.\n",
       "                <a href=\"http://www.pinkwink.kr\" id=\"pw-link\">PinkWink</a>\n",
       "</p>\n",
       "<p class=\"inner-text second-item\">\n",
       "                Happy Data Science.\n",
       "                <a href=\"https://www.python.org\" id=\"py-link\">Python</a>\n",
       "</p>\n",
       "</div>\n",
       "<p class=\"outer-text first-item\" id=\"second\">\n",
       "<b>\n",
       "                Data Science is funny.\n",
       "            </b>\n",
       "</p>\n",
       "<p class=\"outer-text\">\n",
       "<b>\n",
       "                All I need is Love.\n",
       "            </b>\n",
       "</p>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = list(soup.children)[2] # 리스트의 2번째 요소를 불러와줘\n",
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " <head>\n",
       " <title>Very Simple HTML Code by PinkWink</title>\n",
       " </head>,\n",
       " '\\n',\n",
       " <body>\n",
       " <div>\n",
       " <p class=\"inner-text first-item\" id=\"first\">\n",
       "                 Happy PinkWink.\n",
       "                 <a href=\"http://www.pinkwink.kr\" id=\"pw-link\">PinkWink</a>\n",
       " </p>\n",
       " <p class=\"inner-text second-item\">\n",
       "                 Happy Data Science.\n",
       "                 <a href=\"https://www.python.org\" id=\"py-link\">Python</a>\n",
       " </p>\n",
       " </div>\n",
       " <p class=\"outer-text first-item\" id=\"second\">\n",
       " <b>\n",
       "                 Data Science is funny.\n",
       "             </b>\n",
       " </p>\n",
       " <p class=\"outer-text\">\n",
       " <b>\n",
       "                 All I need is Love.\n",
       "             </b>\n",
       " </p>\n",
       " </body>,\n",
       " '\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(html.children) # html의 자식들(밑의 애들)을 가져와줘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* html 파일의 body만 뽑는 법\n",
    "\n",
    "    1. list로 선언해서 안의 내용부터 출력하기\n",
    "    2. soup.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<body>\n",
       "<div>\n",
       "<p class=\"inner-text first-item\" id=\"first\">\n",
       "                Happy PinkWink.\n",
       "                <a href=\"http://www.pinkwink.kr\" id=\"pw-link\">PinkWink</a>\n",
       "</p>\n",
       "<p class=\"inner-text second-item\">\n",
       "                Happy Data Science.\n",
       "                <a href=\"https://www.python.org\" id=\"py-link\">Python</a>\n",
       "</p>\n",
       "</div>\n",
       "<p class=\"outer-text first-item\" id=\"second\">\n",
       "<b>\n",
       "                Data Science is funny.\n",
       "            </b>\n",
       "</p>\n",
       "<p class=\"outer-text\">\n",
       "<b>\n",
       "                All I need is Love.\n",
       "            </b>\n",
       "</p>\n",
       "</body>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body = list(html.children)[3] # 바디만 따로 담을래\n",
    "body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<body>\n",
       "<div>\n",
       "<p class=\"inner-text first-item\" id=\"first\">\n",
       "                Happy PinkWink.\n",
       "                <a href=\"http://www.pinkwink.kr\" id=\"pw-link\">PinkWink</a>\n",
       "</p>\n",
       "<p class=\"inner-text second-item\">\n",
       "                Happy Data Science.\n",
       "                <a href=\"https://www.python.org\" id=\"py-link\">Python</a>\n",
       "</p>\n",
       "</div>\n",
       "<p class=\"outer-text first-item\" id=\"second\">\n",
       "<b>\n",
       "                Data Science is funny.\n",
       "            </b>\n",
       "</p>\n",
       "<p class=\"outer-text\">\n",
       "<b>\n",
       "                All I need is Love.\n",
       "            </b>\n",
       "</p>\n",
       "</body>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.body # 바로 body 보여줘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `soup.find_all()` : 모두 찾아줘\n",
    "* `soup.find()` : 하나만 찾아줘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"inner-text first-item\" id=\"first\">\n",
       "                 Happy PinkWink.\n",
       "                 <a href=\"http://www.pinkwink.kr\" id=\"pw-link\">PinkWink</a>\n",
       " </p>,\n",
       " <p class=\"inner-text second-item\">\n",
       "                 Happy Data Science.\n",
       "                 <a href=\"https://www.python.org\" id=\"py-link\">Python</a>\n",
       " </p>,\n",
       " <p class=\"outer-text first-item\" id=\"second\">\n",
       " <b>\n",
       "                 Data Science is funny.\n",
       "             </b>\n",
       " </p>,\n",
       " <p class=\"outer-text\">\n",
       " <b>\n",
       "                 All I need is Love.\n",
       "             </b>\n",
       " </p>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('p')   # p 태그를 모두 찾아줘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"outer-text first-item\" id=\"second\">\n",
       " <b>\n",
       "                 Data Science is funny.\n",
       "             </b>\n",
       " </p>,\n",
       " <p class=\"outer-text\">\n",
       " <b>\n",
       "                 All I need is Love.\n",
       "             </b>\n",
       " </p>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('p', class_='outer-text')  # p 중 class가 outer-text인 태그만 모두 가져와줘\n",
    "#class가 아닌 점 유의!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* class 명으로 태그 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"outer-text first-item\" id=\"second\">\n",
       " <b>\n",
       "                 Data Science is funny.\n",
       "             </b>\n",
       " </p>,\n",
       " <p class=\"outer-text\">\n",
       " <b>\n",
       "                 All I need is Love.\n",
       "             </b>\n",
       " </p>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(class_='outer-text') # class가 outer-text인 것을 모두 찾아줘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"inner-text first-item\" id=\"first\">\n",
       "                 Happy PinkWink.\n",
       "                 <a href=\"http://www.pinkwink.kr\" id=\"pw-link\">PinkWink</a>\n",
       " </p>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(id=\"first\")   #id가 first인 것을 모두 찾아줘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `soup.head()` : head 내용 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<title>Very Simple HTML Code by PinkWink</title>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.head.next_sibling # soup.head의 다음 애"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<body>\n",
       "<div>\n",
       "<p class=\"inner-text first-item\" id=\"first\">\n",
       "                Happy PinkWink.\n",
       "                <a href=\"http://www.pinkwink.kr\" id=\"pw-link\">PinkWink</a>\n",
       "</p>\n",
       "<p class=\"inner-text second-item\">\n",
       "                Happy Data Science.\n",
       "                <a href=\"https://www.python.org\" id=\"py-link\">Python</a>\n",
       "</p>\n",
       "</div>\n",
       "<p class=\"outer-text first-item\" id=\"second\">\n",
       "<b>\n",
       "                Data Science is funny.\n",
       "            </b>\n",
       "</p>\n",
       "<p class=\"outer-text\">\n",
       "<b>\n",
       "                All I need is Love.\n",
       "            </b>\n",
       "</p>\n",
       "</body>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.head.next_sibling.next_sibling # soup.head의 다음 다음 애"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* html의 텍스트 정보 추출하기\n",
    "\n",
    "    ex. 'p' 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"inner-text first-item\" id=\"first\">\n",
       "                 Happy PinkWink.\n",
       "                 <a href=\"http://www.pinkwink.kr\" id=\"pw-link\">PinkWink</a>\n",
       " </p>,\n",
       " <p class=\"inner-text second-item\">\n",
       "                 Happy Data Science.\n",
       "                 <a href=\"https://www.python.org\" id=\"py-link\">Python</a>\n",
       " </p>,\n",
       " <p class=\"outer-text first-item\" id=\"second\">\n",
       " <b>\n",
       "                 Data Science is funny.\n",
       "             </b>\n",
       " </p>,\n",
       " <p class=\"outer-text\">\n",
       " <b>\n",
       "                 All I need is Love.\n",
       "             </b>\n",
       " </p>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('p') # [] 안에 내용이 있는 것을 보니 리스트 형태~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Happy PinkWink.\n",
      "                PinkWink\n",
      "\n",
      "\n",
      "                Happy Data Science.\n",
      "                Python\n",
      "\n",
      "\n",
      "\n",
      "                Data Science is funny.\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "                All I need is Love.\n",
      "            \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for each_tag in soup.find_all('p'): # p를 모두 찾은 리스트 안을 도는 동안\n",
    "    print(each_tag.get_text())  # 태그 안의 텍스트만 가져와줘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n                Happy PinkWink.\\n                PinkWink\\n\\n\\n                Happy Data Science.\\n                Python\\n\\n\\n\\n\\n                Data Science is funny.\\n            \\n\\n\\n\\n                All I need is Love.\\n            \\n\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body.get_text()  # 태그가 있는 자리는 줄바꿈이 되고 전체 텍스트를 보여줘요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ex. 'a' 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"http://www.pinkwink.kr\" id=\"pw-link\">PinkWink</a>,\n",
       " <a href=\"https://www.python.org\" id=\"py-link\">Python</a>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = soup.find_all('a')\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PinkWink\n",
      "PinkWink -> http://www.pinkwink.kr\n",
      "Python\n",
      "Python -> https://www.python.org\n"
     ]
    }
   ],
   "source": [
    "for each in links: \n",
    "    href = each['href'] # href만 가져와\n",
    "    text = each.string # each에 있는 문자열\n",
    "    print(text + ' -> ' + href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 urllib 라이브러리를 이용해 웹 크롤링하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 앞서 배운 BeautifulSoup 모듈과 urllib 라이브러리를 활용해 실제 사이트에서 웹을 크롤링해볼게요~! \n",
    "\n",
    "* **urllib** 라이브러리란 ?\n",
    "\n",
    "urllib은 URL 처리에 관련된 모듈을 모아 놓은 패키지입니다. urllib에 있는 request 모듈을 가져오고, 그 안의 함수를 사용할거에요. urllib.request.urlopen은 URL을 여는 함수인데 URL 열기에 성공하면 response.status의 값이 200이 나옵니다. 이 200은 HTTP 상태 코드이며 웹 서버가 요청을 제대로 처리했다는 뜻입니다. [더 자세한 내용은 클릭](https://docs.python.org/ko/3/library/urllib.html)\n",
    "\n",
    "네이버 증권 국내증시에서 오늘의 코스피를 가져와 볼겁니다.\n",
    "\n",
    "먼저 코스피에 해당하는 텍스트의 태그, 아이디, 클라스 등을 알아야겠죠? **크롬의 개발자 도구**를 활용하면 쉽게 내가 가져오고자 하는 텍스트의 태그와 아이디, 클라스를 알 수 있어요. 교재 136-138쪽을 보시면 아실 수 있습니다. 혹은 인터넷을 사용해도 되구요.\n",
    "\n",
    "   * **크롬 개발자 도구**?\n",
    "    구글에서 만든 웹브라우저인 크롬에는 개발을 도와주는 다양한 도구가 기본적으로 제공됩니다. \n",
    "    [크롬 개발자 도구 사용법](https://mainia.tistory.com/2393) 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [네이버 금융 국내증시](https://finance.naver.com/sise/)\n",
    "\n",
    "코스피를 크롬 개발자 도구로 찾아보니까 `<span>`에 대한 태그들을 가져와야 겠네요. 그리고 `KOSPI_now` id를 가지고 있어요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "[<span class=\"num \" id=\"KOSPI_now\">2,217.86</span>, <span class=\"num \" id=\"KOSDAQ_now\">801.23</span>, <span class=\"num \" id=\"KPI200_now\">293.51</span>]\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen # request 모듈에서 urlopen import 할게~\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = urlopen('https://finance.naver.com/sise/') # url 넣기\n",
    "print(res.status) # 200이 나오면 성공~\n",
    "soup = BeautifulSoup(res, \"html.parser\") # 객체 생성\n",
    "\n",
    "data = soup.find_all('span', class_='num') # 원하는 태크를 이용해 찾기\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data의 형태가 리스트인 것을 아시겠나요?! 코스피만 가져오려면 가장 첫 항목만 가져오면 되겠죠? 출력해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<span class=\"num \" id=\"KOSPI_now\">2,217.86</span>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kospi = data[0]\n",
    "kospi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2,217.86'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kospi.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_text()를 이용해 코스피만 딱 가져왔네요 :) ~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `get.text()`\n",
    "\n",
    ": 텍스트 내용만 가져와주는 bs4의 메서드입니다.\n",
    "\n",
    "If you only want the human-readable text inside a document or tag, you can use the get_text() method. It returns all the text in a document or beneath a tag, as a single Unicode string.\n",
    "\n",
    "cf. 아까는 함수가 나오고...지금은 메서드...[메서드와 함수의 차이점은 무엇인가요?](https://velog.io/@qoszino/%ED%95%A8%EC%88%98%EC%99%80-%EB%A9%94%EC%86%8C%EB%93%9C%EC%9D%98-%EC%B0%A8%EC%9D%B4%EC%A0%90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 뉴스 기사 타이틀 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 뉴스 기사 타이틀 크롤링하는 것에 대해 알아보려고 해요. 한국일보의 사회 면의 1페이지부터 5페이지까지의 뉴스 타이틀만 가져와볼겁니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [한국일보 사회](https://www.hankookilbo.com/News/Society)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 모듈을 import 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.hankookilbo.com/News/Society/HC01'\n",
    "page = urlopen(url)\n",
    "\n",
    "soup = BeautifulSoup(page, \"html.parser\")\n",
    "\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "soup의 내용을 그냥 print하면 내용이 엄청 많아요. 저희가 찾고자하는 타이틀이 어떤 태그와 클라스를 가지는지 찾아봅시다. 아까 알려드린 구글 개발자 도구를 사용해보세요! 제가 찾아보니 h3 태그이네요 ㅎㅎ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['검찰개혁위 “검찰총장 수사지휘권 폐지, 고검장에 분산” 권고', '경찰, 성폭행 조사받는 탈북자에 한달 간 연락 한번 안 했다', '바다 위에 멈춘 삼척 케이블카', '군위서 통합신공항 공동후보지 찬성 목소리도 많다', '가습기 살균제 총피해자 67만명 추산… 파악된 환자의 100배', '광주대, 2021학년도 수시모집 면접 비대면 진행', '박원순 \\'성추행 방임 의혹\\' 비서실장 등 소환임박... 경찰 \"빠짐없이 모두 조사\"', '비위 알렸더니 제보자 조사해 징계... 과기부 보복성 감사 논란', '경북 의성군 국방부에 \"통합신공항 공동후보지 선정 결단\" 촉구', '법원 \"채널A 기자 휴대폰\\r\\nㆍ노트북 압수수색 취소\"']\n"
     ]
    }
   ],
   "source": [
    "title = soup.find_all('h3')\n",
    "titles=[]\n",
    "\n",
    "for n in title:\n",
    "    a = n.get_text()\n",
    "    titles.append(a)\n",
    "    \n",
    "print(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "잉? 근데 중간 중간에 '\\', '\\r', '\\n' 이런 애들이 있네요. 원래 제목이 아니니까 처리해줘야겠죠? 이런 애들을 골라서 처리해주기 위해 배우는 것이 바로바로 **정규표현식**이랍니다~ 이번 주차 학습을 마치면 깨끗한 제목을 출력하실 수 있을거에요 ㅎㅎ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검찰개혁위 “검찰총장 수사지휘권 폐지, 고검장에 분산” 권고\n",
      "경찰, 성폭행 조사받는 탈북자에 한달 간 연락 한번 안 했다\n",
      "바다 위에 멈춘 삼척 케이블카\n",
      "군위서 통합신공항 공동후보지 찬성 목소리도 많다\n",
      "가습기 살균제 총피해자 67만명 추산… 파악된 환자의 100배\n",
      "광주대, 2021학년도 수시모집 면접 비대면 진행\n",
      "박원순 '성추행 방임 의혹' 비서실장 등 소환임박... 경찰 \"빠짐없이 모두 조사\"\n",
      "비위 알렸더니 제보자 조사해 징계... 과기부 보복성 감사 논란\n",
      "경북 의성군 국방부에 \"통합신공항 공동후보지 선정 결단\" 촉구\n",
      "법원 \"채널A 기자 휴대폰\r\n",
      "ㆍ노트북 압수수색 취소\"\n",
      "꾸지람하는 아버지 흉기 찔러 숨지게 한 20대...법원 중형 선고\n",
      "\"협박에 악플, 퇴사까지\" ...\r\n",
      "한 '몰카 범죄' 피해자에게 벌어진 일\n",
      "용인서 8세 여아 견인차 치어 숨져...CCTV에 담긴 그날의 진실\n",
      "정정순 국회의원, 선거부정 의혹 사건 전방위 확산\n",
      "'장롱 속 영아 시신' 친모ㆍ동거인 결국 구속 \"범행 중대\"\n",
      "조국 논문 표절 의혹에 서울대 \"위반 맞지만 경미한 정도\" 결론\n",
      "구급차 막은 택시기사 결국 구속... 법원 \"범죄 혐의 소명됐다\"\n",
      "제주 해상서 승객 270명 태운 여객선 화재…자체 진화\n",
      "'IQ 210' 김웅용 교수, 의정부시에 책 1만5000권 기증\n",
      "부산 폭우 지하차도 3명 사망, 통제 안 한 책임 누구?\n",
      "\"부산 물난리났는데 노래나 나오고... KBS, 지방은 안중에도 없나\"\n",
      "'경비원 폭행' 주민 변호인, 돌연 사임… 또 미뤄진 재판\n",
      "부산항 러시아 선원발 국내 코로나 감염자 확산\n",
      "구급차 막은 택시기사, 유족에게 하고 싶은 말 묻자 \"뭘\"\n",
      "김창룡 신임 경찰청장 \"개혁은 시대정신이자 국민 명령\"\n",
      "한밤 기습 폭우에 부산이 당했다…3명 사망, 이재민 50여명 발생\n",
      "잠기고 무너지고 깔렸는데 ... \"25일까지 부산에 비 더 온다\"\n",
      "역대급 장대비 맞은 부산 ... 결국 2명 숨져\n",
      "이철-한동훈-이동재 첫 대면... 수사심의위 쟁점은\n",
      "경찰, 박원순 피해자 고소 내용 유출자 파악\n",
      "서울시 기관장, 김재련 변호사에 “살의 느껴… 애도할 시간도 안줘”\n",
      "경찰, '장롱 영아 시신' 관련 20대 2명 체포\n",
      "수사권조정안 '법무장관 수사 승인권' 두고 검경 동시 반발\n",
      "조선대, 2학기 수업 대면+비대면 혼합방식 진행\n",
      "부산 PC방서 10대 여성  ‘묻지마’ 흉기 난동 3명 부상\n",
      "박원순 아이폰 포렌식  시작…  \"피해자 측이 비밀번호 제보\"\n",
      "구급차 막아선 택시기사 구속영장 청구... 고의사고 혐의도\n",
      "결국 합동조사단 계획 철회한 서울시 \"인권위 조사에 적극 협조\"\n",
      "용인 물류센터 화재, '폭발' 아닌 '냉동창고'서 시작\n",
      "한동훈-이동재 녹음파일 공개... 고의적 편집 흔적은 없어\n",
      "코로나에 빼앗긴 문화생활 재개 첫날... 사람들의 웃음도 돌아왔다\n",
      "박원순 피해자 “비서실 20명이 피해 호소 묵살했다”\n",
      "[단독] 박주신 '코로나 유증상자'라 신속 검사... 박능후 해명 거짓이었다\n",
      "이번엔 채용비리 의혹… 광주형 일자리 1대 주주 민낯\n",
      "항소 기각되자 “판사 죽이겠다” 욕설한 살인범\n",
      "박원순 피해자 \"편견 없이, 적법하게 진상 밝혀지길\" 입장문 공개\n",
      "[속보] 박원순 피해자 측 \"경찰 고소 전, 검찰에 문의하며 피고소인 말해\"\n",
      "[속보] 박원순 피해자 측 \"서울시 20여명 피해 방조...조사 주체 돼선 안 돼\"\n",
      "박원순 '성추행 방조 의혹' 서울시청 압수영장, 법원서 기각\n",
      "'42명 사상' 남원 사매2터널 사고 관련자 6명 기소의견 검찰 송치\n"
     ]
    }
   ],
   "source": [
    "titles=[]\n",
    "for n in range(1,6):\n",
    "    url = 'https://www.hankookilbo.com/News/Society/HC01?Page=' + str(n)\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    title = soup.find_all('h3')\n",
    "    for n in title:\n",
    "        a = n.get_text()\n",
    "        titles.append(a)\n",
    "        \n",
    "for title in titles:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 정규표현식\n",
    "정규 표현식(Regular Expressions)은 복잡한 문자열을 처리할 때 사용하는 기법으로, 파이썬만의 고유 문법이 아니라 문자열을 처리하는 모든 곳에서 사용해요. 기초 문법 할 때는 다루지 않았는데요, 그만큼 초보들이 배우기에는 어려울 수 있어요. 하지만 우리는 때가 되었으니! 정규표현식에 대해 배워봅시다.\n",
    "\n",
    "[점프 투 파이썬 7장 정규표현식](https://wikidocs.net/1669) - 7장을 모두 숙지하시면 됩니다.\n",
    "위를 학습한 후, **구글 드라이브의 regularexpression.html을 학습**하세요. 그 안에 구체적인 과제들이 담겨 있습니다. 교안 맨 뒷부분의 도전 과제 2개와 과제 1개를 모두 완성해 해당 구글 콜랩 or 쥬피터 html 파일을 과제로 제출해주시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 openxypl 모듈로 웹 크롤링 자료 저장, 읽기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "크롤링 했다면 그 결과를 담아야 나중에도 또 쓸 수 있겠죠? 후에 판다스에 연결해서 엑셀을 읽어낼 수도 있구요. 이렇게 엑셀에 무언가를 담기 위해 필요한 모듈은 openpyxl이라는 것인데요, 아래 코드를 한번만 이해한다면 계속해서 활용하실 수 있을 거에요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링 데이터로 엑셀파일 만들기 \n",
    "\n",
    "import openpyxl\n",
    "\n",
    "excel_file = openpyxl.Workbook()\n",
    "excel_file.remove(excel_file.active)\n",
    "excel_sheet = excel_file.create_sheet('안녕 시트')  # sheet 이름 작성 \n",
    "\n",
    "excel_sheet.column_dimensions['B'].width = 150 # column B 크기 정하기 \n",
    "\n",
    "for index in range(9):\n",
    "    excel_sheet.append([index, '안녕']) # 엑셀파일에서 이게 어떻게 구현됐을까요? \n",
    "\n",
    "cell_A1 = excel_sheet['A1']\n",
    "cell_A1.alignment = openpyxl.styles.Alignment(horizontal=\"center\")\n",
    "\n",
    "excel_file.save('test.xlsx')  # 엑셀 파일 이름 설정\n",
    "excel_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파일이 저장된 폴더에 가보세요. 그렇다면 test.xlsx 파일이 만들어져 있을 것이랍니다!\n",
    "\n",
    "우리 앞에서 한국일보 타이틀을 추출한 것을 엑셀에다 담아볼까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**과제: 한국일보 사회 지면 5 페이지 크롤링**\n",
    "\n",
    "여기서 과젭니다! 우리는 1페이지만 크롤링을 해봤어요. 그런데 그 이상 여러개의 쪽들을 모두 크롤링하려면 어떻게 해야 할까요? 한국일보 사회면의 1쪽부터 5쪽까지 타이틀만 크롤링해 엑셀 파일에 저장하는 것이 여러분들에게 드리는 과제입니다! 만든 엑셀 파일을 구글 드라이브 과제방에 올려주시면 됩니다.\n",
    "\n",
    "힌트: https://www.hankookilbo.com/News/Society/HC01?Page=1 여기 page에 할당된 숫자를 변경해보세요. 변경한 후 어떤 페이지가 나오는지 생각해보세요.\n",
    "힌트 이외에는 여러분이 프로그래밍으로 충분히 해결할 수 있으니 고민해보세요😁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
